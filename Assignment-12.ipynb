{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习上课内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.回答以下理论问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 请写一下TF-IDF的计算公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf表示单词在一个句子或文档中出现的次数，idf=log_10⁡(𝑁/(𝑑𝑓_𝑡 ))，如果一个词再很多句子或文档中出现则不重要。\n",
    "TF-IDF=tf×idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LDA算法的基本假设是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设1：每个文档的主题服从一个概率分布；\n",
    "假设2：每个主题的词也服从一个分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 在TextRank算法中构建图的权重是如何得到的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个词向量之间余弦相似度即构建图的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 什么是命名实体识别？ 有什么应用场景？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从一句话中识别出人名，地名，组织名，日期时间，这就是命名实体识别的一个例子，而人名，地名等这些被识别的目标就是命名实体。命名实体识别是多分类任务，应用场景：从一句话中识别出人名、地名，从电商的搜索中识别出产品的名字，识别药物名称等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.NLP主要有哪几类任务 ？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类任务：包括文本分类、情感分析等；\n",
    "\n",
    "生成任务：包括机器翻译、问答系统（常识问答和阅读理解、图像问答等）；\n",
    "\n",
    "文本摘要：包括抽取式和生成式，抽取式可能会造成语意不连贯，生成式则可能不稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 手动实现TextRank算法 (在新闻数据中随机提取100条新闻训练词向量和做做法测试）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 提示：\n",
    " 1. 确定窗口，建立图链接。   \n",
    " 2. 通过词向量相似度确定图上边的权重\n",
    " 3. 根据公式实现算法迭代(d=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim import corpora, models\n",
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return re.findall(r'[\\d|\\w]+',string)\n",
    "\n",
    "content = pd.read_csv('xinhua_news.csv',encoding='gb18030')\n",
    "articles = content['content'][:100].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "with open('baidu_stopwords.txt',encoding='utf-8') as f:\n",
    "    for word in f.readlines():\n",
    "        stop_words.append(word.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = ['n', 'nr', 'ns', 'nt', 'eng', 'v', 'd']\n",
    "with open('dict_news','w',encoding='gb18030') as n:\n",
    "    for i in range(len(articles)):\n",
    "        article =' '.join(token(str(articles[i])))\n",
    "        article = article.strip() \n",
    "        seg_words = pseg.cut(article)\n",
    "            #print(article)\n",
    "        article = ' '.join(s.word for s in seg_words if s.flag in flags and s.word not in stop_words) \n",
    "        n.write(article+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec,Word2Vec\n",
    "#sentence = word2vec.LineSentence('dict_news.txt')\n",
    "sentences = word2vec.Text8Corpus('dict_news1.txt')\n",
    "model=word2vec.Word2Vec(sentences,size=300,window=5,min_count=0)\n",
    "model.save('model_gram.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Word2Vec.load('model_gram.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是6月18日在葡萄牙中部大佩德罗冈地区拍摄的被森林大火烧毁的汽车。新华社记者张立云摄\n",
      "\n",
      "[('汽车', 0.2821610740599017), ('记者', 0.26283673671530094), ('地区', 0.19240165513938956), ('烧毁', 0.1739810275917192), ('佩德罗', 0.17151191169026292), ('拍摄', 0.17130581891236818), ('新华社', 0.1695974340778483), ('大火', 0.1648526969559842), ('葡萄牙', 0.1599515697182538), ('张立云', 0.14997005290940393)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TextRank(object):\n",
    "    \n",
    "    def __init__(self, sentence, window, alpha, iternum):\n",
    "        self.sentence = sentence\n",
    "        self.window = window\n",
    "        self.alpha = alpha\n",
    "        self.edge_dict = {} #记录节点的边连接字典\n",
    "        self.iternum = iternum#迭代次数\n",
    " \n",
    "    #对句子进行分词\n",
    "    def cutSentence(self):\n",
    "        jieba.load_userdict('dict_news1.txt')\n",
    "        tag_filter = ['n', 'nr', 'ns', 'nt', 'eng', 'v', 'd']\n",
    "        seg_result = pseg.cut(self.sentence)\n",
    "        self.word_list = [s.word for s in seg_result if s.flag in tag_filter and s.word not in stop_words]\n",
    "        #print(self.word_list)\n",
    " \n",
    "    #根据窗口，构建每个节点的相邻节点,返回边的集合\n",
    "    def createNodes(self):\n",
    "        tmp_list = []\n",
    "        word_list_len = len(self.word_list)\n",
    "        for index, word in enumerate(self.word_list):\n",
    "            if word not in self.edge_dict.keys():\n",
    "                tmp_list.append(word)\n",
    "                tmp_set = set()\n",
    "                left = index - self.window + 1#窗口左边界\n",
    "                right = index + self.window#窗口右边界\n",
    "                if left < 0: left = 0\n",
    "                if right >= word_list_len: right = word_list_len\n",
    "                for i in range(left, right):\n",
    "                    if i == index:\n",
    "                        continue\n",
    "                    tmp_set.add(self.word_list[i])\n",
    "                self.edge_dict[word] = tmp_set\n",
    " \n",
    "    #根据边的相连关系，构建矩阵\n",
    "    def createMatrix(self):\n",
    "        self.matrix = np.zeros([len(set(self.word_list)), len(set(self.word_list))])\n",
    "        self.word_index = {}#记录词的index\n",
    "        self.index_dict = {}#记录节点index对应的词\n",
    " \n",
    "        for i, v in enumerate(set(self.word_list)):\n",
    "            self.word_index[v] = i\n",
    "            self.index_dict[i] = v\n",
    "        for key in self.edge_dict.keys():\n",
    "            for w in self.edge_dict[key]:\n",
    "                self.matrix[self.word_index[key]][self.word_index[w]] = model.wv.similarity(key,w)\n",
    "                self.matrix[self.word_index[w]][self.word_index[key]] = model.wv.similarity(key,w)\n",
    "        #print('matrix',self.matrix)\n",
    "        \n",
    "        \n",
    "    #根据textrank公式计算权重\n",
    "    def calPR(self):\n",
    "        self.PR = np.ones([len(set(self.word_list)), 1])\n",
    "        for i in range(self.iternum):\n",
    "            self.PR = (1 - self.alpha) + self.alpha * np.dot(self.matrix, self.PR)\n",
    " \n",
    "    #输出词和相应的权重\n",
    "    def printResult(self):\n",
    "        word_pr = {}\n",
    "        for i in range(len(self.PR)):\n",
    "            word_pr[self.index_dict[i]] = self.PR[i][0]\n",
    "        res = sorted(word_pr.items(), key = lambda x : x[1], reverse=True)\n",
    "        print(res[:10])\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    #for i in range(len(articles))\n",
    "    print(articles[3])\n",
    "    s = articles[3]\n",
    "    tr = TextRank(s, 3, 0.85, 2000)\n",
    "    tr.cutSentence()\n",
    "    tr.createNodes()\n",
    "    tr.createMatrix()\n",
    "    tr.calPR()\n",
    "    tr.printResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选做 1.  提取新闻人物里的对话。(使用以上提取小数据即可）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：    \n",
    "1.寻找预料里具有表示说的意思。    \n",
    "2.使用语法分析提取句子结构。    \n",
    "3.检测谓语是否有表示说的意思。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择2. ： 电影评论分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个作业中你要完成一个电影评论分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.数据获取。（采用爬虫技术爬取相关网页上的电影评论数据，例如猫眼电影评论，豆瓣电影评论）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.把所获得数据分解为训练集，验证集和测试集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.选用相应算法构建模型，并测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择3：文章自动续写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个作业中你要完成一个文章自动续写的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.数据获取。（根据你的兴趣采用爬虫技术爬去相关网站上的文本数据内容：比如故事网站，小说网站等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.选取模型，并训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.展示一些你模型的输出例子。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
