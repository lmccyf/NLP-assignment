{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.å¤ä¹ ä¸Šè¯¾å†…å®¹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.å›ç­”ä»¥ä¸‹ç†è®ºé—®é¢˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. è¯·å†™ä¸€ä¸‹TF-IDFçš„è®¡ç®—å…¬å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfè¡¨ç¤ºå•è¯åœ¨ä¸€ä¸ªå¥å­æˆ–æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œidf=log_10â¡(ğ‘/(ğ‘‘ğ‘“_ğ‘¡ ))ï¼Œå¦‚æœä¸€ä¸ªè¯å†å¾ˆå¤šå¥å­æˆ–æ–‡æ¡£ä¸­å‡ºç°åˆ™ä¸é‡è¦ã€‚\n",
    "TF-IDF=tfÃ—idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LDAç®—æ³•çš„åŸºæœ¬å‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‡è®¾1ï¼šæ¯ä¸ªæ–‡æ¡£çš„ä¸»é¢˜æœä»ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼›\n",
    "å‡è®¾2ï¼šæ¯ä¸ªä¸»é¢˜çš„è¯ä¹Ÿæœä»ä¸€ä¸ªåˆ†å¸ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. åœ¨TextRankç®—æ³•ä¸­æ„å»ºå›¾çš„æƒé‡æ˜¯å¦‚ä½•å¾—åˆ°çš„ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸¤ä¸ªè¯å‘é‡ä¹‹é—´ä½™å¼¦ç›¸ä¼¼åº¦å³æ„å»ºå›¾çš„æƒé‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ä»€ä¹ˆæ˜¯å‘½åå®ä½“è¯†åˆ«ï¼Ÿ æœ‰ä»€ä¹ˆåº”ç”¨åœºæ™¯ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»ä¸€å¥è¯ä¸­è¯†åˆ«å‡ºäººåï¼Œåœ°åï¼Œç»„ç»‡åï¼Œæ—¥æœŸæ—¶é—´ï¼Œè¿™å°±æ˜¯å‘½åå®ä½“è¯†åˆ«çš„ä¸€ä¸ªä¾‹å­ï¼Œè€Œäººåï¼Œåœ°åç­‰è¿™äº›è¢«è¯†åˆ«çš„ç›®æ ‡å°±æ˜¯å‘½åå®ä½“ã€‚å‘½åå®ä½“è¯†åˆ«æ˜¯å¤šåˆ†ç±»ä»»åŠ¡ï¼Œåº”ç”¨åœºæ™¯ï¼šä»ä¸€å¥è¯ä¸­è¯†åˆ«å‡ºäººåã€åœ°åï¼Œä»ç”µå•†çš„æœç´¢ä¸­è¯†åˆ«å‡ºäº§å“çš„åå­—ï¼Œè¯†åˆ«è¯ç‰©åç§°ç­‰ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.NLPä¸»è¦æœ‰å“ªå‡ ç±»ä»»åŠ¡ ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ†ç±»ä»»åŠ¡ï¼šåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æç­‰ï¼›\n",
    "\n",
    "ç”Ÿæˆä»»åŠ¡ï¼šåŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿï¼ˆå¸¸è¯†é—®ç­”å’Œé˜…è¯»ç†è§£ã€å›¾åƒé—®ç­”ç­‰ï¼‰ï¼›\n",
    "\n",
    "æ–‡æœ¬æ‘˜è¦ï¼šåŒ…æ‹¬æŠ½å–å¼å’Œç”Ÿæˆå¼ï¼ŒæŠ½å–å¼å¯èƒ½ä¼šé€ æˆè¯­æ„ä¸è¿è´¯ï¼Œç”Ÿæˆå¼åˆ™å¯èƒ½ä¸ç¨³å®šã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.å®è·µé¢˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 æ‰‹åŠ¨å®ç°TextRankç®—æ³• (åœ¨æ–°é—»æ•°æ®ä¸­éšæœºæå–100æ¡æ–°é—»è®­ç»ƒè¯å‘é‡å’Œåšåšæ³•æµ‹è¯•ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " æç¤ºï¼š\n",
    " 1. ç¡®å®šçª—å£ï¼Œå»ºç«‹å›¾é“¾æ¥ã€‚   \n",
    " 2. é€šè¿‡è¯å‘é‡ç›¸ä¼¼åº¦ç¡®å®šå›¾ä¸Šè¾¹çš„æƒé‡\n",
    " 3. æ ¹æ®å…¬å¼å®ç°ç®—æ³•è¿­ä»£(d=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim import corpora, models\n",
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return re.findall(r'[\\d|\\w]+',string)\n",
    "\n",
    "content = pd.read_csv('xinhua_news.csv',encoding='gb18030')\n",
    "articles = content['content'][:100].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "with open('baidu_stopwords.txt',encoding='utf-8') as f:\n",
    "    for word in f.readlines():\n",
    "        stop_words.append(word.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = ['n', 'nr', 'ns', 'nt', 'eng', 'v', 'd']\n",
    "with open('dict_news','w',encoding='gb18030') as n:\n",
    "    for i in range(len(articles)):\n",
    "        article =' '.join(token(str(articles[i])))\n",
    "        article = article.strip() \n",
    "        seg_words = pseg.cut(article)\n",
    "            #print(article)\n",
    "        article = ' '.join(s.word for s in seg_words if s.flag in flags and s.word not in stop_words) \n",
    "        n.write(article+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec,Word2Vec\n",
    "#sentence = word2vec.LineSentence('dict_news.txt')\n",
    "sentences = word2vec.Text8Corpus('dict_news1.txt')\n",
    "model=word2vec.Word2Vec(sentences,size=300,window=5,min_count=0)\n",
    "model.save('model_gram.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Word2Vec.load('model_gram.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¿™æ˜¯6æœˆ18æ—¥åœ¨è‘¡è„ç‰™ä¸­éƒ¨å¤§ä½©å¾·ç½—å†ˆåœ°åŒºæ‹æ‘„çš„è¢«æ£®æ—å¤§ç«çƒ§æ¯çš„æ±½è½¦ã€‚æ–°åç¤¾è®°è€…å¼ ç«‹äº‘æ‘„\n",
      "\n",
      "[('æ±½è½¦', 0.2821610740599017), ('è®°è€…', 0.26283673671530094), ('åœ°åŒº', 0.19240165513938956), ('çƒ§æ¯', 0.1739810275917192), ('ä½©å¾·ç½—', 0.17151191169026292), ('æ‹æ‘„', 0.17130581891236818), ('æ–°åç¤¾', 0.1695974340778483), ('å¤§ç«', 0.1648526969559842), ('è‘¡è„ç‰™', 0.1599515697182538), ('å¼ ç«‹äº‘', 0.14997005290940393)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TextRank(object):\n",
    "    \n",
    "    def __init__(self, sentence, window, alpha, iternum):\n",
    "        self.sentence = sentence\n",
    "        self.window = window\n",
    "        self.alpha = alpha\n",
    "        self.edge_dict = {} #è®°å½•èŠ‚ç‚¹çš„è¾¹è¿æ¥å­—å…¸\n",
    "        self.iternum = iternum#è¿­ä»£æ¬¡æ•°\n",
    " \n",
    "    #å¯¹å¥å­è¿›è¡Œåˆ†è¯\n",
    "    def cutSentence(self):\n",
    "        jieba.load_userdict('dict_news1.txt')\n",
    "        tag_filter = ['n', 'nr', 'ns', 'nt', 'eng', 'v', 'd']\n",
    "        seg_result = pseg.cut(self.sentence)\n",
    "        self.word_list = [s.word for s in seg_result if s.flag in tag_filter and s.word not in stop_words]\n",
    "        #print(self.word_list)\n",
    " \n",
    "    #æ ¹æ®çª—å£ï¼Œæ„å»ºæ¯ä¸ªèŠ‚ç‚¹çš„ç›¸é‚»èŠ‚ç‚¹,è¿”å›è¾¹çš„é›†åˆ\n",
    "    def createNodes(self):\n",
    "        tmp_list = []\n",
    "        word_list_len = len(self.word_list)\n",
    "        for index, word in enumerate(self.word_list):\n",
    "            if word not in self.edge_dict.keys():\n",
    "                tmp_list.append(word)\n",
    "                tmp_set = set()\n",
    "                left = index - self.window + 1#çª—å£å·¦è¾¹ç•Œ\n",
    "                right = index + self.window#çª—å£å³è¾¹ç•Œ\n",
    "                if left < 0: left = 0\n",
    "                if right >= word_list_len: right = word_list_len\n",
    "                for i in range(left, right):\n",
    "                    if i == index:\n",
    "                        continue\n",
    "                    tmp_set.add(self.word_list[i])\n",
    "                self.edge_dict[word] = tmp_set\n",
    " \n",
    "    #æ ¹æ®è¾¹çš„ç›¸è¿å…³ç³»ï¼Œæ„å»ºçŸ©é˜µ\n",
    "    def createMatrix(self):\n",
    "        self.matrix = np.zeros([len(set(self.word_list)), len(set(self.word_list))])\n",
    "        self.word_index = {}#è®°å½•è¯çš„index\n",
    "        self.index_dict = {}#è®°å½•èŠ‚ç‚¹indexå¯¹åº”çš„è¯\n",
    " \n",
    "        for i, v in enumerate(set(self.word_list)):\n",
    "            self.word_index[v] = i\n",
    "            self.index_dict[i] = v\n",
    "        for key in self.edge_dict.keys():\n",
    "            for w in self.edge_dict[key]:\n",
    "                self.matrix[self.word_index[key]][self.word_index[w]] = model.wv.similarity(key,w)\n",
    "                self.matrix[self.word_index[w]][self.word_index[key]] = model.wv.similarity(key,w)\n",
    "        #print('matrix',self.matrix)\n",
    "        \n",
    "        \n",
    "    #æ ¹æ®textrankå…¬å¼è®¡ç®—æƒé‡\n",
    "    def calPR(self):\n",
    "        self.PR = np.ones([len(set(self.word_list)), 1])\n",
    "        for i in range(self.iternum):\n",
    "            self.PR = (1 - self.alpha) + self.alpha * np.dot(self.matrix, self.PR)\n",
    " \n",
    "    #è¾“å‡ºè¯å’Œç›¸åº”çš„æƒé‡\n",
    "    def printResult(self):\n",
    "        word_pr = {}\n",
    "        for i in range(len(self.PR)):\n",
    "            word_pr[self.index_dict[i]] = self.PR[i][0]\n",
    "        res = sorted(word_pr.items(), key = lambda x : x[1], reverse=True)\n",
    "        print(res[:10])\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    #for i in range(len(articles))\n",
    "    print(articles[3])\n",
    "    s = articles[3]\n",
    "    tr = TextRank(s, 3, 0.85, 2000)\n",
    "    tr.cutSentence()\n",
    "    tr.createNodes()\n",
    "    tr.createMatrix()\n",
    "    tr.calPR()\n",
    "    tr.printResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### é€‰åš 1.  æå–æ–°é—»äººç‰©é‡Œçš„å¯¹è¯ã€‚(ä½¿ç”¨ä»¥ä¸Šæå–å°æ•°æ®å³å¯ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æç¤ºï¼š    \n",
    "1.å¯»æ‰¾é¢„æ–™é‡Œå…·æœ‰è¡¨ç¤ºè¯´çš„æ„æ€ã€‚    \n",
    "2.ä½¿ç”¨è¯­æ³•åˆ†ææå–å¥å­ç»“æ„ã€‚    \n",
    "3.æ£€æµ‹è°“è¯­æ˜¯å¦æœ‰è¡¨ç¤ºè¯´çš„æ„æ€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### é€‰æ‹©2. ï¼š ç”µå½±è¯„è®ºåˆ†ç±»ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨è¿™ä¸ªä½œä¸šä¸­ä½ è¦å®Œæˆä¸€ä¸ªç”µå½±è¯„è®ºåˆ†ç±»ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.æ•°æ®è·å–ã€‚ï¼ˆé‡‡ç”¨çˆ¬è™«æŠ€æœ¯çˆ¬å–ç›¸å…³ç½‘é¡µä¸Šçš„ç”µå½±è¯„è®ºæ•°æ®ï¼Œä¾‹å¦‚çŒ«çœ¼ç”µå½±è¯„è®ºï¼Œè±†ç“£ç”µå½±è¯„è®ºï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.æŠŠæ‰€è·å¾—æ•°æ®åˆ†è§£ä¸ºè®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.é€‰ç”¨ç›¸åº”ç®—æ³•æ„å»ºæ¨¡å‹ï¼Œå¹¶æµ‹è¯•ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### é€‰æ‹©3ï¼šæ–‡ç« è‡ªåŠ¨ç»­å†™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨è¿™ä¸ªä½œä¸šä¸­ä½ è¦å®Œæˆä¸€ä¸ªæ–‡ç« è‡ªåŠ¨ç»­å†™çš„æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.æ•°æ®è·å–ã€‚ï¼ˆæ ¹æ®ä½ çš„å…´è¶£é‡‡ç”¨çˆ¬è™«æŠ€æœ¯çˆ¬å»ç›¸å…³ç½‘ç«™ä¸Šçš„æ–‡æœ¬æ•°æ®å†…å®¹ï¼šæ¯”å¦‚æ•…äº‹ç½‘ç«™ï¼Œå°è¯´ç½‘ç«™ç­‰ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.é€‰å–æ¨¡å‹ï¼Œå¹¶è®­ç»ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.å±•ç¤ºä¸€äº›ä½ æ¨¡å‹çš„è¾“å‡ºä¾‹å­ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
